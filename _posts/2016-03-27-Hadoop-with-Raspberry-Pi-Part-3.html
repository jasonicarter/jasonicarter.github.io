---
layout: post
title:  "How to Hadoop at home with Raspberry Pi - Part 3"
date: 2016-03-27
tags: [raspberry pi, hadoop, tutorial]
comments: true
share: true
---

<section name="6c56" class=" section--body"><div class="section-content"><div class="section-inner layoutSingleColumn"><p name="9dbe" id="9dbe" class="graf--p graf--leading">In the home stretch now to completing my Raspberry Pi Hadoop cluster. But first a quick summary. I started this personal project about 3 weeks ago because of my interest in “Big Data”, data analytics and data engineering. I’m also taking <a href="https://www.udacity.com/nanodegrees-new-s/nd002" data-href="https://www.udacity.com/nanodegrees-new-s/nd002" class="markup--anchor markup--p-anchor" rel="nofollow">Udacity’s Nanodegree in Data Analytics</a> so I figured getting my hands dirty with Hadoop was a great way to dive in. That being said, this is Part 3 of 3, I’ve setup Raspberry Pi in <a href="{{ site.url }}/Hadoop-with-Raspberry-Pi-Part-1" class="markup--anchor markup--p-anchor">part 1</a> then installed and configured Hadoop in a single node configuration in <a href="{{ site.url }}/Hadoop-with-Raspberry-Pi-Part-2" class="markup--anchor markup--p-anchor">part 2</a>. Now it’s time to get the cluster working.</p><p name="9e92" id="9e92" class="graf--p graf-after--p"><strong class="markup--strong markup--p-strong">This is not a tutorial.</strong> Think of it more as a journey, there’s no nice step-by-step process here, I’m going to make mistakes, get errors, fix them and try to move on.</p><ol class="postList"><li name="756b" id="756b" class="graf--li graf-after--p"><a href="{{ site.url }}/Hadoop-with-Raspberry-Pi-Part-1" class="markup--anchor markup--li-anchor"><em class="markup--em markup--li-em">Part 1: Setting up Raspberry Pi and network configurations</em></a></li><li name="6795" id="6795" class="graf--li graf-after--li"><a href="{{ site.url }}/Hadoop-with-Raspberry-Pi-Part-2" class="markup--anchor markup--li-anchor"><em class="markup--em markup--li-em">Part 2: Hadoop single node setup, testing and prepping the cluster</em></a></li><li name="3a0b" id="3a0b" class="graf--li graf-after--li graf--last"><a href="{{ site.url }}/Hadoop-with-Raspberry-Pi-Part-3" class="markup--anchor markup--li-anchor"><em class="markup--em markup--li-em">Part 3: Hadoop cluster setup, testing and final thoughts</em></a></li></ol></div></div></section><section name="0da4" class=" section--body"><div class="section-divider layoutSingleColumn"><hr class="section-divider"></div><div class="section-content"><div class="section-inner layoutSingleColumn"><h3 name="74a1" id="74a1" class="graf--h3 graf--leading">Hadoop and distributed data processing</h3><h4 name="6ae3" id="6ae3" class="graf--h4 graf-after--h3">Now that we’re talking…</h4><p name="b4d9" id="b4d9" class="graf--p graf-after--h4">over SSH, it’s time to start things up and make sure all of our services are running properly on all 3 nodes. I log into node1 and start the services.</p><pre name="b865" id="b865" class="graf--pre graf-after--p">1. $su hduser<br>2. $cd $HADOOP_HOME/sbin<br>3. $./start-dfs.sh<br>4. $./start-yarn.sh</pre><p name="38fe" id="38fe" class="graf--p graf-after--pre">From node1, the services are up and running but let us see what’s happening on node 2 and 3.</p><pre name="57c7" id="57c7" class="graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">NodeManager and DataNode should be running on both nodes<br></strong>$ssh node2 (node3)<br>$jps</pre><p name="a44a" id="a44a" class="graf--p graf-after--pre">On node 2, services are running but not on node 3. I could start things directly on node 3 but I shouldn’t have to so it’s time to stop and re-start both dfs and YARN (executed from node1).</p><p name="a8a6" id="a8a6" class="graf--p graf-after--p">Still nothing. Repeated a few times, need to do some research.</p><p name="391e" id="391e" class="graf--p graf-after--p">The problem: dataNode service is only running on one or two nodes and sometimes the service is starting up and then shutting down automatically.</p><p name="84d1" id="84d1" class="graf--p graf-after--p">The Solution: Strangely, it’s pretty much the same as I encountered before when formatting the nameNode. I believe copying the SD Card and the nameNode fiasco resulting in my cloned cards having out of sync ClusterIDs. So again, I just followed the steps below, then restarted everything. Once done, all services across each node were running and stable. Yay!</p><pre name="3ce7" id="3ce7" class="graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">Delete hdfs storage, add permissions and repeat for all nodes</strong><br>1. rm -rf /opt/hadoop/hadoop_data<br>2. $sudo mkdir -p /opt/hadoop/hadoop_data/hdfs/namenode (not required for nodes 2 and 3)<br>3. $sudo mkdir -p /opt/hadoop/hadoop_data/hdfs/datanode<br>4. $sudo chown hduser:hadoop /opt/hadoop/hadoop_data/hdfs -R<br>5. $sudo chmod 750 /opt/hadoop/hadoop_data/hdfs</pre><pre name="e191" id="e191" class="graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">Format node1 namenode</strong><br>$hdfs namenode -format</pre><pre name="bd23" id="bd23" class="graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">Start up hadoop on node 1 and run jps to validate things are running</strong><br>1. $cd $HADOOP_HOME/sbin<br>2. $./start-dfs.sh<br>3. $./start-yarn.sh<br>4. $jps (test on all nodes)</pre><h4 name="c7fc" id="c7fc" class="graf--h4 graf-after--pre">Testing…</h4><p name="d5b9" id="d5b9" class="graf--p graf-after--h4">your creation is probably the most exciting and frustrating part of any personal project. Now that I have my cluster actually running I need to do a few tests to make sure things are working as expected.</p><p name="3cae" id="3cae" class="graf--p graf-after--p">Similar to the single node test, I’m going to try the wordCount and Pi examples.</p><pre name="5b4b" id="5b4b" class="graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">Copy the file,check HDFS for the file then run wordCount on the file</strong></pre><pre name="cbb3" id="cbb3" class="graf--pre graf-after--pre">1. $hdfs dfs -copyFromLocal /opt/hadoop/LICENSE.txt /license.txt<br>2. $./hadoop jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /license.txt /license</pre><p name="07a6" id="07a6" class="graf--p graf-after--pre">The job is running but it seems like everything is running on node1. To be 100% sure about this, I can always do some network and CPU monitoring. Raspberry Pi comes pre-installed with such a tool called top. However, I heard <a href="http://nmon.sourceforge.net/pmwiki.php" data-href="http://nmon.sourceforge.net/pmwiki.php" class="markup--anchor markup--p-anchor" rel="nofollow">nmon</a> is pretty good, if not better, so time to test it out.</p><pre name="a5c3" id="a5c3" class="graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">Install nmon and run it on each node</strong><br>$sudo apt-get install nmon<br>$nmon</pre><pre name="2ea3" id="2ea3" class="graf--pre graf-after--pre">I have c(CPU),m(memory) and n(network) options open across terminals</pre><p name="f40c" id="f40c" class="graf--p graf-after--pre">Now time to rerun and see what’s happening…Yup, only node1 is doing work, I also figured out how to view the individual tasks in the YARN web UI, default 8088 port.</p><p name="42c1" id="42c1" class="graf--p graf-after--p">So with things only running on node1 there are a couple of reasons and solutions for this, that I know.</p><ol class="postList"><li name="27b2" id="27b2" class="graf--li graf-after--p">All blocks of the file are on one dataNode. And this happened because I uploaded the file locally to HDFS but the local node I’m on (node1) is also acting/configured as a dataNode. And with replication set to 1, and Hadoop running jobs close to the data, everything gets processed on node1.</li><li name="3d24" id="3d24" class="graf--li graf-after--li">Block size is too big. When I uploaded the test file into HDFS it breaks the file into blocks which it then replicates across servers. However, if the block is 10MB but the file is 1MB there’s no need to break up the file, meaning only 1 block is required and hence no distributed processing.</li></ol><p name="22e6" id="22e6" class="graf--p graf-after--li">Both of these are easy to fix. First, I need a bigger test file and secondly, I need to change some configurations across the nodes.</p><pre name="10fd" id="10fd" class="graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">Get test file</strong><br>Downloaded a 35MB text file for testing</pre><pre name="7f4d" id="7f4d" class="graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">Remove node1 as a dataNode</strong><br>$sudo nano /opt/hadoop/etc/hadoop/slaves<br>remove: node1</pre><pre name="9ba4" id="9ba4" class="graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">Update block size (hdfs-site.xml)</strong><br>35MB test file isn&#39;t that big so add a block size parameter of 5MB, forcing Hadoop to break up the file.<br>&lt;property&gt;<br>  &lt;name&gt;dfs.block.size&lt;/name&gt;<br>  &lt;value&gt;5242880&lt;/value&gt;<br>&lt;/property&gt;</pre><pre name="db6e" id="db6e" class="graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">Update replication (hdfs-site.xml)<br></strong>&lt;property&gt;<br>  &lt;name&gt;dfs.replication&lt;/name&gt;<br>  &lt;value&gt;2&lt;/value&gt;<br>&lt;/property&gt;</pre><p name="68ea" id="68ea" class="graf--p graf-after--pre">Now in my system I have a nameNode (node1) and 2 dataNodes (node2 and node3). Time to re-test…And re-test again and again. Data is being replicated across the 2 dataNodes but now the job isn’t running at all. The CLI is stuck on “INFO mapreduce.Job: Running job: job_145…”</p><p name="ab63" id="ab63" class="graf--p graf-after--p">Time for more investigation. And when reviewing the logs, or the YARN web UI, I keep seeing:</p><pre name="aaa5" id="aaa5" class="graf--pre graf-after--p">Accepted: waiting for AM container to be allocated...</pre><p name="1183" id="1183" class="graf--p graf-after--pre">The job is basically stuck in “Accepted” state. On SO there seems to be a lot of different “solutions” to this problem. The solution that worked for me now seems so obvious…I’m using YARN, a resource manager, its configuration is local and used by each node in the cluster but each node in the cluster needs to know where the resource manager is within the cluster.</p><p name="91dd" id="91dd" class="graf--p graf-after--p">Therefore, time for another configuration update of the yarn-site.xml file.</p><pre name="054b" id="054b" class="graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">Update yarn-site.xml on all nodes with the resource manager hostname<br></strong>&lt;property&gt;<br>  &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;<br>  &lt;value&gt;node1&lt;/value&gt;<br>&lt;/property&gt;</pre><p name="ae4d" id="ae4d" class="graf--p graf-after--pre">Now YARN knows that node1 is the resource manager, but more importantly, node2 and node3 also knows who the resource manager is. Start things up again, and this time go to node1:8088 in the browser and you should see both node2 and node3 listed as active and running.</p><h4 name="6d23" id="6d23" class="graf--h4 graf-after--p">Testing the redux…</h4><p name="4ad0" id="4ad0" class="graf--p graf-after--h4">will be an error free event, fingers crossed. With what appears to be a working distributed system, it’s time to test Hadoop with the wordCount example one last time.</p><pre name="2e1f" id="2e1f" class="graf--pre graf-after--p">$./yarn jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /testFile.txt /testFile</pre><pre name="311a" id="311a" class="graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">Note:</strong> notice I used &quot;yarn&quot; instead of &quot;hadoop&quot; to run the job, this is another test. I noticed this change in a few articles and I&#39;m still not sure what the difference is but both seem to work the same</pre><p name="77bd" id="77bd" class="graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">SUCCESS!</strong></p><p name="2ae5" id="2ae5" class="graf--p graf-after--p">With nmon monitoring all 3 nodes and also reviewing the tasks via browser, you can see how the map jobs were shared among both node2 and node3. In <a href="https://medium.com/@jasonicarter/how-to-hadoop-at-home-with-raspberry-pi-part-2-b8ccfbe6ba9a#.ikaq2e3v9" data-href="https://medium.com/@jasonicarter/how-to-hadoop-at-home-with-raspberry-pi-part-2-b8ccfbe6ba9a#.ikaq2e3v9" class="markup--anchor markup--p-anchor">part 2</a>, I recorded a few timed runs of Pi on the single node, and now with a distributed cluster, I should see some better times…Sounds like another test is coming.</p><pre name="f961" id="f961" class="graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">hadoop-mapreduce-examples-2.7.2.jar pi 10 10<br></strong>(Estimated value of Pi is 3.200)</pre><pre name="7534" id="7534" class="graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">Original Pi tests:<br></strong>Job Finished in 346.618 seconds<br>Job Finished in 392.751 seconds<br>Job Finished in 370.214 seconds<br>Job Finished in 361.656 seconds</pre><pre name="b502" id="b502" class="graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">Distributed Pi tests:</strong><br>Job Finished in 142.409 seconds<br>Job Finished in 164.503 seconds<br>Job Finished in 122.628 seconds<br>Job Finished in 133.072 seconds<br>Job Finished in 116.997 seconds</pre><p name="aa15" id="aa15" class="graf--p graf-after--pre">It is just me, or is <strong class="markup--strong markup--p-strong">117 seconds</strong> a beautiful thing to see? With the cluster working, data processing is distributed across the nodes in parallel and working more efficiently. Yay! Finally, I have a Raspberry Pi Hadoop cluster.</p><h3 name="4577" id="4577" class="graf--h3 graf-after--p">Final Thoughts</h3><h4 name="57d4" id="57d4" class="graf--h4 graf-after--h3">Damn…</h4><p name="bf5b" id="bf5b" class="graf--p graf-after--h4">that was involved. I didn’t think it would be easy but with so many articles and tutorials out there, I definitely didn’t think I’d run into so many issues. Part of that, I believe, was due to the new version of Hadoop. With Hadoop 2.x, a number of things changed such as system/folder paths, the introduction of YARN, MapReduce 2 and MapReduce configuration changes to integrate with YARN. Not to mention this is running on a RaspberryPi, memory and CPU resources were a problem as well.</p><p name="c226" id="c226" class="graf--p graf-after--p">For my final thoughts, I wont get into a bunch of technical “gotcha” moments, instead since this was a personal journey, here are my personal thoughts…</p><ul class="postList"><li name="8b7d" id="8b7d" class="graf--li graf-after--p">Building a Hadoop cluster on Raspberry Pi was an amazing learning and satisfying experience</li><li name="2ac8" id="2ac8" class="graf--li graf-after--li">So much more to learn. Hadoop configurations will make or break your cluster’s performance — or even prevent it from working in the first place</li><li name="58a1" id="58a1" class="graf--li graf-after--li">I would love to bring 117 down too 100 seconds with optimizing Hadoop configuration, so we’ll see how that goes</li><li name="b774" id="b774" class="graf--li graf-after--li">I hope my experience here will add some benefit to the library of online Hadoop 2 + Raspberry Pi articles.</li><li name="c6fa" id="c6fa" class="graf--li graf-after--li">Only the beginning. Going to setup Hive next! And then Pig.</li></ul><p name="d45d" id="d45d" class="graf--p graf-after--li"><em class="markup--em markup--p-em">Update: I’ve had a few requests about providing a step-by-step setup and final working config files. And after reading over my 3 articles, I think that’s definitely a reasonable request, so look out for it in the near future.</em></p><p name="be4b" id="be4b" class="graf--p graf-after--p graf--last"><strong class="markup--strong markup--p-strong">RaspberryPi Hadoop cluster. BigData on a small scale.</strong></p></div></div></section><section name="6a4f" class=" section--body section--last"><div class="section-divider layoutSingleColumn"><hr class="section-divider"></div><div class="section-content"><div class="section-inner layoutSingleColumn"><p name="bd64" id="bd64" class="graf--p graf--leading">Questions? Comments? Let me know. Thanks for following my journey.</p><p name="3a93" id="3a93" class="graf--p graf-after--p">Articles that helped me along…</p><ul class="postList"><li name="8dc5" id="8dc5" class="graf--li graf-after--p"><a href="http://www.widriksson.com/raspberry-pi-hadoop-cluster/" data-href="http://www.widriksson.com/raspberry-pi-hadoop-cluster/" class="markup--anchor markup--li-anchor" rel="nofollow">http://www.widriksson.com/raspberry-pi-hadoop-cluster/</a></li><li name="fbbc" id="fbbc" class="graf--li graf-after--li"><a href="https://www.raspberrypi.org/downloads/noobs/" data-href="https://www.raspberrypi.org/downloads/noobs/" class="markup--anchor markup--li-anchor" rel="nofollow">https://www.raspberrypi.org/downloads/noobs/</a></li><li name="9a78" id="9a78" class="graf--li graf-after--li"><a href="http://blog.ittoby.com/2013/08/starting-small-set-up-hadoop-compute.html" data-href="http://blog.ittoby.com/2013/08/starting-small-set-up-hadoop-compute.html" class="markup--anchor markup--li-anchor" rel="nofollow">http://blog.ittoby.com/2013/08/starting-small-set-up-hadoop-compute.html</a></li><li name="77c3" id="77c3" class="graf--li graf-after--li"><a href="http://computers.tutsplus.com/articles/how-to-clone-raspberry-pi-sd-cards-using-the-command-line-in-os-x--mac-59911" data-href="http://computers.tutsplus.com/articles/how-to-clone-raspberry-pi-sd-cards-using-the-command-line-in-os-x--mac-59911" class="markup--anchor markup--li-anchor" rel="nofollow">http://computers.tutsplus.com/articles/how-to-clone-raspberry-pi-sd-cards-using-the-command-line-in-os-x--mac-59911</a></li><li name="6d26" id="6d26" class="graf--li graf-after--li"><a href="https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html" data-href="https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html" class="markup--anchor markup--li-anchor" rel="nofollow">https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html</a></li><li name="25a9" id="25a9" class="graf--li graf-after--li"><a href="http://bradhedlund.com/2011/09/10/understanding-hadoop-clusters-and-the-network/" data-href="http://bradhedlund.com/2011/09/10/understanding-hadoop-clusters-and-the-network/" class="markup--anchor markup--li-anchor" rel="nofollow">http://bradhedlund.com/2011/09/10/understanding-hadoop-clusters-and-the-network/</a></li><li name="4647" id="4647" class="graf--li graf-after--li graf--last"><a href="http://hortonworks.com/hadoop/" data-href="http://hortonworks.com/hadoop/" class="markup--anchor markup--li-anchor" rel="nofollow">http://hortonworks.com/hadoop/</a></li></ul></div></div></section>
</section>
<footer><p><a href="https://medium.com/p/7d114d35fdf1">View the original on Medium</a></p></footer></article>
